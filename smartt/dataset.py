"""
PyTorch Dataset implementation for tensor tomography reconstructions.

This module provides a Dataset class for loading and accessing tensor tomography
reconstruction data stored in HDF5 format.
"""

import h5py
import numpy as np
import torch
from pathlib import Path
from typing import Union, Optional, Tuple, Dict, Any
from torch.utils.data import Dataset


class ReconstructionDataset(Dataset):
    """
    PyTorch Dataset for tensor tomography reconstructions.
    
    This dataset loads 4D reconstruction tensors from an HDF5 file generated by
    the data_processing module. Each item can be a sparse reconstruction alone
    or a pair of (sparse_reconstruction, ground_truth) depending on the 
    return_ground_truth parameter.
    
    Parameters
    ----------
    hdf5_path : str or Path
        Path to the HDF5 file containing reconstruction data.
    transform : callable, optional
        Optional transform to apply to each reconstruction tensor.
    load_in_memory : bool, default=False
        If True, loads all reconstructions into memory at initialization.
        If False, reads from disk on each __getitem__ call (slower but uses less memory).
    return_metadata : bool, default=False
        If True, __getitem__ returns a tuple (tensor, metadata_dict) or
        ((sparse, ground_truth), metadata_dict) if return_ground_truth=True.
        If False, __getitem__ returns only the tensor(s).
    return_ground_truth : bool, default=True
        If True, __getitem__ returns (sparse_reconstruction, ground_truth).
        If False, returns only sparse_reconstruction.
    granularity : str, default='coarse'
        Controls the granularity of data returned:
        - 'coarse': Returns full 4D tensors of shape (H, W, D, num_coefficients)
        - 'fine': Flattens spatial dimensions and returns individual coefficient vectors
          of shape (num_coefficients,). Dataset length becomes num_samples * H * W * D.
    
    Attributes
    ----------
    num_samples : int
        Total number of sparse reconstructions in the dataset.
    num_ground_truths : int
        Number of ground truth reconstructions (one per input file).
    volume_shape : tuple
        Shape of the 3D volume (height, width, depth).
    num_coefficients : int
        Number of spherical harmonic coefficients.
    ell_max : int
        Maximum spherical harmonic degree.
    num_projections : int
        Number of projections used for sparse reconstructions.
    file_identifiers : list
        List of source file identifiers.
    granularity : str
        Either 'coarse' (full 4D tensors) or 'fine' (individual coefficient vectors).
    num_voxels : int
        Total number of voxels per reconstruction (H * W * D).
        Only relevant when granularity='fine'.
    
    Examples
    --------
    >>> from smartt import ReconstructionDataset
    >>> from torch.utils.data import DataLoader
    >>> 
    >>> # Load dataset with ground truth pairs (coarse granularity)
    >>> dataset = ReconstructionDataset('training_data.h5', return_ground_truth=True)
    >>> print(f"Dataset size: {len(dataset)}")
    >>> print(f"Volume shape: {dataset.volume_shape}")
    >>> 
    >>> # Get a single sample (returns sparse and ground truth)
    >>> sparse, ground_truth = dataset[0]
    >>> print(f"Sparse shape: {sparse.shape}")  # e.g., (65, 82, 65, 45)
    >>> print(f"Ground truth shape: {ground_truth.shape}")
    >>> 
    >>> # Load dataset with fine granularity (individual voxels)
    >>> fine_dataset = ReconstructionDataset('training_data.h5', 
    ...                                      return_ground_truth=True,
    ...                                      granularity='fine')
    >>> print(f"Fine dataset size: {len(fine_dataset)}")  # 65*82*65 times larger
    >>> sparse_vec, gt_vec = fine_dataset[0]
    >>> print(f"Sparse vector shape: {sparse_vec.shape}")  # (45,)
    >>> 
    >>> # Use with DataLoader 
    >>> dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    >>> for sparse_batch, gt_batch in dataloader:
    ...     print(f"Sparse batch shape: {sparse_batch.shape}")
    ...     print(f"GT batch shape: {gt_batch.shape}")
    ...     break
    """
    
    def __init__(
        self,
        hdf5_path: Union[str, Path],
        transform: Optional[callable] = None,
        load_in_memory: bool = False,
        return_metadata: bool = False,
        return_ground_truth: bool = True,
        granularity: str = 'coarse'
    ):
        """Initialize the ReconstructionDataset."""
        self.hdf5_path = Path(hdf5_path)
        self.transform = transform
        self.load_in_memory = load_in_memory
        self.return_metadata = return_metadata
        self.return_ground_truth = return_ground_truth
        self.granularity = granularity
        
        if granularity not in ['coarse', 'fine']:
            raise ValueError(f"granularity must be 'coarse' or 'fine', got '{granularity}'")
        
        if not self.hdf5_path.exists():
            raise FileNotFoundError(f"HDF5 file not found: {self.hdf5_path}")
        
        # Load metadata
        with h5py.File(self.hdf5_path, 'r') as f:
            self.num_samples = f['reconstructions'].shape[0]
            self.volume_shape = tuple(f['reconstructions'].shape[1:4])
            self.num_coefficients = f['reconstructions'].shape[4]
            self.ell_max = int(f['ell_max'][()])
            self.num_projections = int(f['num_projections'][()])
            
            # Calculate number of voxels per reconstruction
            self.num_voxels = int(np.prod(self.volume_shape))
            
            # Load ground truth information if available
            if 'ground_truths' in f:
                self.has_ground_truth = True
                self.num_ground_truths = f['ground_truths'].shape[0]
                self.reconstruction_to_gt = f['reconstruction_to_gt_index'][:]
            else:
                self.has_ground_truth = False
                self.num_ground_truths = 0
                self.reconstruction_to_gt = None
                if return_ground_truth:
                    print("Warning: return_ground_truth=True but no ground truths found in file. "
                          "Will return only sparse reconstructions.")
            
            # Load file identifiers if available
            if 'file_identifiers' in f:
                self.file_identifiers = [s.decode('utf-8') if isinstance(s, bytes) else s 
                                        for s in f['file_identifiers'][:]]
            else:
                self.file_identifiers = None
            
            # Load additional metadata if available
            self.metadata = {}
            if 'num_iterations' in f:
                self.metadata['num_iterations'] = int(f['num_iterations'][()])
            if 'maxiter' in f:
                self.metadata['maxiter'] = int(f['maxiter'][()])
            if 'regularization_weight' in f:
                self.metadata['regularization_weight'] = float(f['regularization_weight'][()])
            if 'data_path' in f.attrs:
                self.metadata['source_data_path'] = f.attrs['data_path']
            if 'num_files' in f.attrs:
                self.metadata['num_files'] = int(f.attrs['num_files'])
            
            # Optionally load projection indices
            if 'projection_indices' in f:
                self.projection_indices = f['projection_indices'][:]
            else:
                self.projection_indices = None
            
            # Load all data into memory if requested
            if self.load_in_memory:
                self.data = f['reconstructions'][:]
                if self.has_ground_truth:
                    self.ground_truth_data = f['ground_truths'][:]
                else:
                    self.ground_truth_data = None
            else:
                self.data = None
                self.ground_truth_data = None
    
    def __len__(self) -> int:
        """Return the number of reconstructions in the dataset."""
        if self.granularity == 'fine':
            # In fine granularity mode, return total number of voxels across all samples
            return self.num_samples * self.num_voxels
        else:
            # In coarse granularity mode, return number of reconstruction volumes
            return self.num_samples
    
    def __getitem__(self, idx: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], 
                                              Tuple[torch.Tensor, Dict[str, Any]], 
                                              Tuple[Tuple[torch.Tensor, torch.Tensor], Dict[str, Any]]]:
        """
        Get a reconstruction tensor by index.
        
        Parameters
        ----------
        idx : int
            Index of the reconstruction to retrieve.
            - In 'coarse' mode: Index of the full 4D reconstruction volume.
            - In 'fine' mode: Linear index into all voxels across all samples.
        
        Returns
        -------
        torch.Tensor or tuple
            Depending on granularity, return_ground_truth and return_metadata:
            
            Coarse granularity:
            - If return_ground_truth=False, return_metadata=False: 
              Returns torch.Tensor of sparse reconstruction with shape (H, W, D, num_coefficients)
            - If return_ground_truth=True, return_metadata=False:
              Returns (sparse_tensor, ground_truth_tensor) both with shape (H, W, D, num_coefficients)
            - If return_ground_truth=False, return_metadata=True:
              Returns (sparse_tensor, metadata_dict)
            - If return_ground_truth=True, return_metadata=True:
              Returns ((sparse_tensor, ground_truth_tensor), metadata_dict)
            
            Fine granularity:
            - Returns coefficient vector(s) of shape (num_coefficients,)
            - Metadata includes 'voxel_coords' (i, j, k) and 'sample_index'
        """
        if self.granularity == 'coarse':
            return self._get_coarse_item(idx)
        else:  # granularity == 'fine'
            return self._get_fine_item(idx)
    
    def _get_coarse_item(self, idx: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], 
                                                    Tuple[torch.Tensor, Dict[str, Any]], 
                                                    Tuple[Tuple[torch.Tensor, torch.Tensor], Dict[str, Any]]]:
        """Get a full 4D reconstruction volume."""
        if idx < 0 or idx >= self.num_samples:
            raise IndexError(f"Index {idx} out of range [0, {self.num_samples})")
        
        # Load sparse reconstruction
        if self.load_in_memory:
            reconstruction = self.data[idx]
        else:
            with h5py.File(self.hdf5_path, 'r') as f:
                reconstruction = f['reconstructions'][idx]
        
        # Convert to torch tensor
        reconstruction = torch.from_numpy(reconstruction.astype(np.float32))
        
        # Apply transform if provided
        if self.transform is not None:
            reconstruction = self.transform(reconstruction)
        
        # Load ground truth if requested
        ground_truth = None
        if self.return_ground_truth and self.has_ground_truth:
            gt_idx = self.reconstruction_to_gt[idx]
            if self.load_in_memory:
                ground_truth = self.ground_truth_data[gt_idx]
            else:
                with h5py.File(self.hdf5_path, 'r') as f:
                    ground_truth = f['ground_truths'][gt_idx]
            
            # Convert to torch tensor
            ground_truth = torch.from_numpy(ground_truth.astype(np.float32))
            
            # Apply same transform to ground truth if provided
            if self.transform is not None:
                ground_truth = self.transform(ground_truth)
        
        # Prepare metadata if requested
        if self.return_metadata:
            item_metadata = {
                'index': idx,
                'num_projections': self.num_projections,
                'ell_max': self.ell_max,
            }
            if self.projection_indices is not None:
                item_metadata['projection_indices'] = self.projection_indices[idx]
            if self.has_ground_truth:
                item_metadata['ground_truth_index'] = int(self.reconstruction_to_gt[idx])
                if self.file_identifiers is not None:
                    item_metadata['file_identifier'] = self.file_identifiers[self.reconstruction_to_gt[idx]]
        
        # Return based on flags
        if self.return_ground_truth and self.has_ground_truth:
            if self.return_metadata:
                return (reconstruction, ground_truth), item_metadata
            else:
                return reconstruction, ground_truth
        else:
            if self.return_metadata:
                return reconstruction, item_metadata
            else:
                return reconstruction
    
    def _get_fine_item(self, idx: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], 
                                                  Tuple[torch.Tensor, Dict[str, Any]], 
                                                  Tuple[Tuple[torch.Tensor, torch.Tensor], Dict[str, Any]]]:
        """Get a single voxel's coefficient vector from the flattened dataset."""
        total_items = self.num_samples * self.num_voxels
        if idx < 0 or idx >= total_items:
            raise IndexError(f"Index {idx} out of range [0, {total_items})")
        
        # Convert linear index to (sample_idx, voxel_idx)
        sample_idx = idx // self.num_voxels
        voxel_idx = idx % self.num_voxels
        
        # Convert voxel_idx to 3D coordinates (i, j, k)
        h, w, d = self.volume_shape
        k = voxel_idx % d
        j = (voxel_idx // d) % w
        i = voxel_idx // (d * w)
        voxel_coords = (i, j, k)
        
        # Load the full reconstruction volume for this sample
        if self.load_in_memory:
            reconstruction = self.data[sample_idx]
        else:
            with h5py.File(self.hdf5_path, 'r') as f:
                reconstruction = f['reconstructions'][sample_idx]
        
        # Extract the coefficient vector for this specific voxel
        coeff_vector = reconstruction[i, j, k, :]
        coeff_vector = torch.from_numpy(coeff_vector.astype(np.float32))
        
        # Apply transform if provided (note: transform might expect 4D, so be careful)
        if self.transform is not None:
            coeff_vector = self.transform(coeff_vector)
        
        # Load ground truth voxel if requested
        ground_truth_vector = None
        if self.return_ground_truth and self.has_ground_truth:
            gt_idx = self.reconstruction_to_gt[sample_idx]
            if self.load_in_memory:
                ground_truth = self.ground_truth_data[gt_idx]
            else:
                with h5py.File(self.hdf5_path, 'r') as f:
                    ground_truth = f['ground_truths'][gt_idx]
            
            # Extract the coefficient vector for this specific voxel
            ground_truth_vector = ground_truth[i, j, k, :]
            ground_truth_vector = torch.from_numpy(ground_truth_vector.astype(np.float32))
            
            # Apply same transform if provided
            if self.transform is not None:
                ground_truth_vector = self.transform(ground_truth_vector)
        
        # Prepare metadata if requested
        if self.return_metadata:
            item_metadata = {
                'index': idx,
                'sample_index': sample_idx,
                'voxel_index': voxel_idx,
                'voxel_coords': voxel_coords,
                'num_projections': self.num_projections,
                'ell_max': self.ell_max,
            }
            if self.projection_indices is not None:
                item_metadata['projection_indices'] = self.projection_indices[sample_idx]
            if self.has_ground_truth:
                item_metadata['ground_truth_index'] = int(self.reconstruction_to_gt[sample_idx])
                if self.file_identifiers is not None:
                    item_metadata['file_identifier'] = self.file_identifiers[self.reconstruction_to_gt[sample_idx]]
        
        # Return based on flags
        if self.return_ground_truth and self.has_ground_truth:
            if self.return_metadata:
                return (coeff_vector, ground_truth_vector), item_metadata
            else:
                return coeff_vector, ground_truth_vector
        else:
            if self.return_metadata:
                return coeff_vector, item_metadata
            else:
                return coeff_vector
    
    def get_metadata(self) -> Dict[str, Any]:
        """
        Get dataset-level metadata.
        
        Returns
        -------
        dict
            Dictionary containing dataset metadata including num_samples,
            volume_shape, num_coefficients, ell_max, num_projections,
            num_ground_truths, file_identifiers, etc.
        """
        meta = {
            'num_samples': self.num_samples,
            'volume_shape': self.volume_shape,
            'num_coefficients': self.num_coefficients,
            'ell_max': self.ell_max,
            'num_projections': self.num_projections,
            'hdf5_path': str(self.hdf5_path),
            'has_ground_truth': self.has_ground_truth,
            'granularity': self.granularity,
            'num_voxels': self.num_voxels,
        }
        if self.granularity == 'fine':
            meta['effective_dataset_length'] = self.num_samples * self.num_voxels
        if self.has_ground_truth:
            meta['num_ground_truths'] = self.num_ground_truths
        if self.file_identifiers is not None:
            meta['file_identifiers'] = self.file_identifiers
        meta.update(self.metadata)
        return meta
    
    def get_ground_truth(self, gt_idx: int) -> torch.Tensor:
        """
        Get a specific ground truth reconstruction by its index.
        
        Parameters
        ----------
        gt_idx : int
            Index of the ground truth reconstruction (0 to num_ground_truths-1).
        
        Returns
        -------
        torch.Tensor
            The ground truth reconstruction tensor.
        """
        if not self.has_ground_truth:
            raise ValueError("This dataset does not contain ground truth reconstructions.")
        
        if gt_idx < 0 or gt_idx >= self.num_ground_truths:
            raise IndexError(f"Ground truth index {gt_idx} out of range [0, {self.num_ground_truths})")
        
        if self.load_in_memory:
            ground_truth = self.ground_truth_data[gt_idx]
        else:
            with h5py.File(self.hdf5_path, 'r') as f:
                ground_truth = f['ground_truths'][gt_idx]
        
        ground_truth = torch.from_numpy(ground_truth.astype(np.float32))
        
        if self.transform is not None:
            ground_truth = self.transform(ground_truth)
        
        return ground_truth
    
    def get_file_identifier(self, idx: int) -> Optional[str]:
        """
        Get the source file identifier for a reconstruction.
        
        Parameters
        ----------
        idx : int
            Index of the reconstruction.
        
        Returns
        -------
        str or None
            File identifier string, or None if not available.
        """
        if not self.has_ground_truth or self.file_identifiers is None:
            return None
        
        if idx < 0 or idx >= self.num_samples:
            raise IndexError(f"Index {idx} out of range [0, {self.num_samples})")
        
        gt_idx = self.reconstruction_to_gt[idx]
        return self.file_identifiers[gt_idx]
    
    def get_projection_indices(self, idx: int) -> Optional[np.ndarray]:
        """
        Get the projection indices used for a specific reconstruction.
        
        Parameters
        ----------
        idx : int
            Index of the reconstruction.
        
        Returns
        -------
        np.ndarray or None
            Array of projection indices, or None if not available.
        """
        if self.projection_indices is None:
            return None
        if idx < 0 or idx >= self.num_samples:
            raise IndexError(f"Index {idx} out of range [0, {self.num_samples})")
        return self.projection_indices[idx]
    
    def __repr__(self) -> str:
        """Return string representation of the dataset."""
        repr_str = (
            f"ReconstructionDataset(\n"
            f"  hdf5_path={self.hdf5_path},\n"
            f"  num_samples={self.num_samples},\n"
            f"  volume_shape={self.volume_shape},\n"
            f"  num_coefficients={self.num_coefficients},\n"
            f"  ell_max={self.ell_max},\n"
            f"  num_projections={self.num_projections},\n"
            f"  load_in_memory={self.load_in_memory},\n"
            f"  return_ground_truth={self.return_ground_truth},\n"
            f"  granularity='{self.granularity}'"
        )
        if self.granularity == 'fine':
            repr_str += f",\n  effective_length={len(self)} ({self.num_samples} Ã— {self.num_voxels})"
        if self.has_ground_truth:
            repr_str += f",\n  num_ground_truths={self.num_ground_truths}"
        if self.file_identifiers is not None:
            repr_str += f",\n  num_files={len(self.file_identifiers)}"
        repr_str += "\n)"
        return repr_str


class ReconstructionDatasetSubset(Dataset):
    """
    A subset of ReconstructionDataset with specific coefficient channels.
    
    This is useful when you want to work with only a subset of spherical harmonic
    coefficients, for example, only the first few orders.
    
    Parameters
    ----------
    base_dataset : ReconstructionDataset
        The base dataset to create a subset from.
    coefficient_indices : array-like
        Indices of coefficients to include in the subset.
    
    Examples
    --------
    >>> dataset = ReconstructionDataset('training_data.h5')
    >>> # Only use first 9 coefficients (up to ell=2)
    >>> subset = ReconstructionDatasetSubset(dataset, coefficient_indices=range(9))
    >>> print(f"Subset shape: {subset[0].shape}")
    """
    
    def __init__(
        self,
        base_dataset: ReconstructionDataset,
        coefficient_indices: Union[list, np.ndarray, range]
    ):
        """Initialize the subset dataset."""
        self.base_dataset = base_dataset
        self.coefficient_indices = np.array(list(coefficient_indices))
        
        # Validate indices
        max_idx = self.base_dataset.num_coefficients
        if np.any(self.coefficient_indices >= max_idx) or np.any(self.coefficient_indices < 0):
            raise ValueError(
                f"Coefficient indices must be in range [0, {max_idx}), "
                f"got range [{self.coefficient_indices.min()}, {self.coefficient_indices.max()}]"
            )
    
    def __len__(self) -> int:
        """Return the number of samples (same as base dataset)."""
        return len(self.base_dataset)
    
    def __getitem__(self, idx: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor],
                                              Tuple[torch.Tensor, Dict], 
                                              Tuple[Tuple[torch.Tensor, torch.Tensor], Dict]]:
        """Get a reconstruction with only selected coefficient channels."""
        result = self.base_dataset[idx]
        
        # Handle different return types from base dataset
        if self.base_dataset.return_ground_truth and self.base_dataset.has_ground_truth:
            if self.base_dataset.return_metadata:
                (reconstruction, ground_truth), metadata = result
                subset_reconstruction = reconstruction[..., self.coefficient_indices]
                subset_ground_truth = ground_truth[..., self.coefficient_indices]
                return (subset_reconstruction, subset_ground_truth), metadata
            else:
                reconstruction, ground_truth = result
                subset_reconstruction = reconstruction[..., self.coefficient_indices]
                subset_ground_truth = ground_truth[..., self.coefficient_indices]
                return subset_reconstruction, subset_ground_truth
        else:
            if self.base_dataset.return_metadata:
                reconstruction, metadata = result
                subset_reconstruction = reconstruction[..., self.coefficient_indices]
                return subset_reconstruction, metadata
            else:
                reconstruction = result
                return reconstruction[..., self.coefficient_indices]
    
    def __repr__(self) -> str:
        """Return string representation."""
        return (
            f"ReconstructionDatasetSubset(\n"
            f"  base_dataset={self.base_dataset.__class__.__name__},\n"
            f"  num_samples={len(self)},\n"
            f"  selected_coefficients={len(self.coefficient_indices)}/{self.base_dataset.num_coefficients}\n"
            f")"
        )


if __name__ == "__main__":
    # Example usage
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python dataset.py <hdf5_file>")
        sys.exit(1)
    
    hdf5_path = sys.argv[1]
    
    print("Loading dataset...")
    dataset = ReconstructionDataset(hdf5_path, return_ground_truth=True)
    print(dataset)
    print("\nMetadata:")
    for key, value in dataset.get_metadata().items():
        print(f"  {key}: {value}")
    
    print(f"\nLoading first sample...")
    result = dataset[0]
    
    if dataset.has_ground_truth and dataset.return_ground_truth:
        sparse, gt = result
        print(f"Sparse reconstruction shape: {sparse.shape}, dtype: {sparse.dtype}")
        print(f"Ground truth shape: {gt.shape}, dtype: {gt.dtype}")
    else:
        print(f"Reconstruction shape: {result.shape}, dtype: {result.dtype}")
    
    if dataset.projection_indices is not None:
        indices = dataset.get_projection_indices(0)
        print(f"\nProjection indices for first sample: {indices}")
    
    if dataset.file_identifiers is not None:
        file_id = dataset.get_file_identifier(0)
        print(f"File identifier for first sample: {file_id}")
