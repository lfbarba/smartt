{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998a47c4",
   "metadata": {},
   "source": [
    "# Sparse Reconstruction Uncertainty Analysis\n",
    "\n",
    "This notebook analyzes the uncertainty in sparse tensor tomography reconstructions by:\n",
    "1. Computing a ground truth reconstruction using all available projections (240 angles)\n",
    "2. Fixing a sparse subset of 60 projections\n",
    "3. Repeatedly sampling 80% of these 60 projections (48 projections)\n",
    "4. Computing k reconstructions and analyzing their standard deviation\n",
    "5. Visualizing the uncertainty across spherical harmonic coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced09733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /das/work/units/pem/p20639/envs/p20639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccb871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Setting the number of threads to 8. If your physical cores are fewer than this number, you may want to use numba.set_num_threads(n), and os.environ[\"OPENBLAS_NUM_THREADS\"] = f\"{n}\" to set the number of threads to the number of physical cores n.\n",
      "INFO:Setting numba log level to WARNING.\n",
      "INFO:Setting numba log level to WARNING.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from mumott.data_handling import DataContainer\n",
    "except:\n",
    "    !sh ../scripts/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51feaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/myhome/smartt')\n",
    "sys.path.insert(0, '/das/home/barbaf_l/smartTT')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# Import mumott modules\n",
    "from mumott.data_handling import DataContainer\n",
    "from mumott.methods.basis_sets import SphericalHarmonics\n",
    "from mumott.methods.projectors import SAXSProjector, SAXSProjectorCUDA\n",
    "from mumott.methods.residual_calculators import GradientResidualCalculator\n",
    "from mumott.optimization.loss_functions import SquaredLoss\n",
    "from mumott.optimization.optimizers import LBFGS\n",
    "from mumott.optimization.regularizers import Laplacian\n",
    "\n",
    "# Import custom functions\n",
    "# from smartt.data_processing import _perform_reconstruction\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0135d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data processing module for generating tensor tomography reconstruction datasets.\n",
    "\n",
    "This module provides functionality to load projection data, randomly sample subsets,\n",
    "perform mumott reconstructions, and save the results for machine learning training.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, List, Tuple, Dict, Literal\n",
    "from mumott.data_handling import DataContainer\n",
    "from mumott.methods.basis_sets import SphericalHarmonics\n",
    "from mumott.methods.projectors import SAXSProjector, SAXSProjectorCUDA\n",
    "from mumott.methods.residual_calculators import GradientResidualCalculator\n",
    "from mumott.optimization.loss_functions import SquaredLoss\n",
    "from mumott.optimization.optimizers import LBFGS\n",
    "from mumott.optimization.regularizers import Laplacian\n",
    "\n",
    "# Import PyTorch projectors and SH forward\n",
    "from smartt.projectors import forward_project, backproject, build_mumott_projector\n",
    "from smartt.shutils.evaulate_sh import forward_quadrature\n",
    "\n",
    "\n",
    "def _perform_reconstruction_lbfgs(\n",
    "    dc: DataContainer,\n",
    "    ell_max: int,\n",
    "    maxiter: int,\n",
    "    regularization_weight: float,\n",
    "    use_cuda: bool,\n",
    "    verbose: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a single spherical harmonic reconstruction using mumott LBFGS.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : DataContainer\n",
    "        The data container with projections.\n",
    "    ell_max : int\n",
    "        Maximum degree for spherical harmonics expansion.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations for the LBFGS optimizer.\n",
    "    regularization_weight : float\n",
    "        Weight for the Laplacian regularization term.\n",
    "    use_cuda : bool\n",
    "        Whether to use CUDA for computation.\n",
    "    verbose : bool, default=False\n",
    "        Whether to print progress information.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The reconstruction as a numpy array of shape (*volume_shape, num_coeffs).\n",
    "    \"\"\"\n",
    "    basis_set = SphericalHarmonics(ell_max=ell_max)\n",
    "    \n",
    "    # Create projector (CUDA or CPU)\n",
    "    if use_cuda:\n",
    "        projector = SAXSProjectorCUDA(dc.geometry)\n",
    "    else:\n",
    "        projector = SAXSProjector(dc.geometry)\n",
    "    \n",
    "    residual_calculator = GradientResidualCalculator(\n",
    "        data_container=dc,\n",
    "        basis_set=basis_set,\n",
    "        projector=projector\n",
    "    )\n",
    "    \n",
    "    loss_function = SquaredLoss(residual_calculator)\n",
    "    regularizer = Laplacian()\n",
    "    loss_function.add_regularizer(\n",
    "        name='laplacian',\n",
    "        regularizer=regularizer,\n",
    "        regularization_weight=regularization_weight\n",
    "    )\n",
    "    \n",
    "    optimizer = LBFGS(loss_function, maxiter=maxiter)\n",
    "    \n",
    "    # Run optimization\n",
    "    if verbose:\n",
    "        print(\"Running LBFGS optimization...\")\n",
    "    results = optimizer.optimize()\n",
    "    \n",
    "    # Extract reconstruction\n",
    "    if isinstance(results, dict):\n",
    "        if 'x' in results:\n",
    "            reconstruction = results['x']\n",
    "        elif 'reconstruction' in results:\n",
    "            reconstruction = results['reconstruction']\n",
    "        elif 'coefficients' in results:\n",
    "            reconstruction = results['coefficients']\n",
    "        else:\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, (np.ndarray, torch.Tensor)) or hasattr(value, 'shape'):\n",
    "                    reconstruction = value\n",
    "                    if verbose:\n",
    "                        print(f\"Using key '{key}' as reconstruction\")\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find reconstruction in results. Keys: {list(results.keys())}\")\n",
    "    elif hasattr(results, 'reconstruction'):\n",
    "        reconstruction = results.reconstruction\n",
    "    else:\n",
    "        reconstruction = results\n",
    "    \n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if torch.is_tensor(reconstruction):\n",
    "        reconstruction = reconstruction.cpu().numpy()\n",
    "    \n",
    "    # Ensure it's a numpy array\n",
    "    if not isinstance(reconstruction, np.ndarray):\n",
    "        reconstruction = np.asarray(reconstruction)\n",
    "    \n",
    "    return reconstruction.astype(np.float32)\n",
    "\n",
    "\n",
    "def _perform_reconstruction_pytorch(\n",
    "    dc: DataContainer,\n",
    "    ell_max: int,\n",
    "    maxiter: int = 500,\n",
    "    lr: float = 0.01,\n",
    "    regularization_weight: float = 0.0,\n",
    "    regularization_type: Literal['none', 'l2', 'tv', 'laplacian'] = 'none',\n",
    "    use_cuda: bool = True,\n",
    "    verbose: bool = False,\n",
    "    log_interval: int = 50,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform spherical harmonic reconstruction using PyTorch AdamW optimization.\n",
    "    \n",
    "    This function uses the PyTorch-accelerated forward projector and spherical\n",
    "    harmonics forward operator to optimize the volume coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : DataContainer\n",
    "        The data container with projections and geometry.\n",
    "    ell_max : int\n",
    "        Maximum degree for spherical harmonics expansion.\n",
    "    maxiter : int, default=500\n",
    "        Maximum number of AdamW iterations.\n",
    "    lr : float, default=0.01\n",
    "        Learning rate for AdamW optimizer.\n",
    "    regularization_weight : float, default=0.0\n",
    "        Weight for regularization term.\n",
    "    regularization_type : str, default='none'\n",
    "        Type of regularization: 'none', 'l2', 'tv' (total variation), 'laplacian'.\n",
    "    use_cuda : bool, default=True\n",
    "        Whether to use CUDA for computation.\n",
    "    verbose : bool, default=False\n",
    "        Whether to print progress information.\n",
    "    log_interval : int, default=50\n",
    "        How often to print loss during optimization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The reconstruction as a numpy array of shape (*volume_shape, num_coeffs).\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Get geometry info\n",
    "    geometry = dc.geometry\n",
    "    volume_shape = geometry.volume_shape\n",
    "    num_coeffs = (ell_max // 2 + 1) * (ell_max + 1)\n",
    "    \n",
    "    # Get target data (measured projections after SH forward)\n",
    "    # The data in dc.projections is already in detector segment space\n",
    "    # Shape: (N, J, K, M) where N=projections, J,K=spatial, M=detector segments\n",
    "    # Note: mumott ProjectionStack needs to be accessed element by element\n",
    "    projection_data = np.stack([p.data for p in dc.projections])\n",
    "    target_data = torch.tensor(\n",
    "        projection_data.astype(np.float32), \n",
    "        device=device, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Target data shape: {target_data.shape}\")\n",
    "        print(f\"Volume shape: {volume_shape}\")\n",
    "        print(f\"Number of SH coefficients: {num_coeffs}\")\n",
    "        print(f\"Device: {device}\")\n",
    "    \n",
    "    # Create basis set for SH forward (get projection matrix from mumott for exact matching)\n",
    "    basis_set = SphericalHarmonics(ell_max=ell_max, probed_coordinates=geometry.probed_coordinates)\n",
    "    projection_matrix = torch.tensor(\n",
    "        basis_set.projection_matrix.astype(np.float32),\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"SH projection matrix shape: {projection_matrix.shape}\")\n",
    "    \n",
    "    # Build differentiable projector (uses autograd with adjoint for backward)\n",
    "    projector = build_mumott_projector(geometry, device=device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Built differentiable projector\")\n",
    "    \n",
    "    # Initialize volume coefficients (learnable parameters)\n",
    "    # Shape: (*volume_shape, num_coeffs)\n",
    "    volume = torch.zeros(\n",
    "        (*volume_shape, num_coeffs),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW([volume], lr=lr, weight_decay=0.0)\n",
    "    \n",
    "    # Optional: learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStarting PyTorch optimization ({maxiter} iterations)...\")\n",
    "        print(f\"Regularization: {regularization_type} (weight={regularization_weight})\")\n",
    "    \n",
    "    for iteration in range(maxiter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward projection: volume -> spatial projections\n",
    "        # projector expects (X, Y, Z, C) and returns (N, J, K, C)\n",
    "        # This uses build_mumott_projector which wraps ASTRA with autograd\n",
    "        spatial_projections = projector(volume)\n",
    "        \n",
    "        # SH forward: spatial projections -> detector segments\n",
    "        # forward_quadrature expects (N, H, W, C) and returns (N, H, W, M)\n",
    "        # Keep projection_matrix on GPU for the einsum operation\n",
    "        predicted = forward_quadrature(\n",
    "            geometry.probed_coordinates,\n",
    "            spatial_projections,\n",
    "            ell_max=ell_max,\n",
    "            projection_matrix=projection_matrix  # Use mumott's projection matrix (torch tensor)\n",
    "        )\n",
    "        \n",
    "        # Data fidelity loss (MSE)\n",
    "        data_loss = torch.nn.functional.mse_loss(predicted, target_data)\n",
    "        \n",
    "        # Regularization\n",
    "        reg_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        if regularization_type == 'l2' and regularization_weight > 0:\n",
    "            # L2 regularization on coefficients\n",
    "            reg_loss = regularization_weight * torch.mean(volume ** 2)\n",
    "            \n",
    "        elif regularization_type == 'tv' and regularization_weight > 0:\n",
    "            # Total variation regularization (anisotropic)\n",
    "            dx = torch.diff(volume, dim=0)\n",
    "            dy = torch.diff(volume, dim=1)\n",
    "            dz = torch.diff(volume, dim=2)\n",
    "            reg_loss = regularization_weight * (\n",
    "                torch.mean(torch.abs(dx)) + \n",
    "                torch.mean(torch.abs(dy)) + \n",
    "                torch.mean(torch.abs(dz))\n",
    "            )\n",
    "            \n",
    "        elif regularization_type == 'laplacian' and regularization_weight > 0:\n",
    "            # Laplacian regularization (smoothness)\n",
    "            # Approximate Laplacian using finite differences\n",
    "            laplacian = torch.zeros_like(volume)\n",
    "            # Interior points\n",
    "            laplacian[1:-1, :, :, :] += volume[:-2, :, :, :] + volume[2:, :, :, :] - 2 * volume[1:-1, :, :, :]\n",
    "            laplacian[:, 1:-1, :, :] += volume[:, :-2, :, :] + volume[:, 2:, :, :] - 2 * volume[:, 1:-1, :, :]\n",
    "            laplacian[:, :, 1:-1, :] += volume[:, :, :-2, :] + volume[:, :, 2:, :] - 2 * volume[:, :, 1:-1, :]\n",
    "            reg_loss = regularization_weight * torch.mean(laplacian ** 2)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = data_loss + reg_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_([volume], max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(total_loss.detach())\n",
    "        \n",
    "        # Logging\n",
    "        losses.append(total_loss.item())\n",
    "        \n",
    "        if verbose and (iteration + 1) % log_interval == 0:\n",
    "            print(f\"  Iter {iteration + 1:4d}: loss={total_loss.item():.6f} \"\n",
    "                  f\"(data={data_loss.item():.6f}, reg={reg_loss.item():.6f})\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nOptimization complete. Final loss: {losses[-1]:.6f}\")\n",
    "    \n",
    "    # Convert to numpy\n",
    "    reconstruction = volume.detach().cpu().numpy()\n",
    "    \n",
    "    return reconstruction.astype(np.float32)\n",
    "\n",
    "\n",
    "# Keep the original function name for backwards compatibility\n",
    "def _perform_reconstruction(\n",
    "    dc: DataContainer,\n",
    "    ell_max: int,\n",
    "    maxiter: int,\n",
    "    regularization_weight: float,\n",
    "    use_cuda: bool,\n",
    "    verbose: bool = False,\n",
    "    method: Literal['lbfgs', 'pytorch'] = 'lbfgs',\n",
    "    **kwargs\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a single spherical harmonic reconstruction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : DataContainer\n",
    "        The data container with projections.\n",
    "    ell_max : int\n",
    "        Maximum degree for spherical harmonics expansion.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations.\n",
    "    regularization_weight : float\n",
    "        Weight for the regularization term.\n",
    "    use_cuda : bool\n",
    "        Whether to use CUDA for computation.\n",
    "    verbose : bool, default=False\n",
    "        Whether to print progress information.\n",
    "    method : str, default='lbfgs'\n",
    "        Optimization method: 'lbfgs' (mumott) or 'pytorch' (AdamW).\n",
    "    **kwargs\n",
    "        Additional arguments for the PyTorch method (lr, regularization_type, etc.)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The reconstruction as a numpy array of shape (*volume_shape, num_coeffs).\n",
    "    \"\"\"\n",
    "    if method == 'lbfgs':\n",
    "        return _perform_reconstruction_lbfgs(\n",
    "            dc=dc,\n",
    "            ell_max=ell_max,\n",
    "            maxiter=maxiter,\n",
    "            regularization_weight=regularization_weight,\n",
    "            use_cuda=use_cuda,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    elif method == 'pytorch':\n",
    "        return _perform_reconstruction_pytorch(\n",
    "            dc=dc,\n",
    "            ell_max=ell_max,\n",
    "            maxiter=maxiter,\n",
    "            regularization_weight=regularization_weight,\n",
    "            use_cuda=use_cuda,\n",
    "            verbose=verbose,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'lbfgs' or 'pytorch'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38a02e",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8953aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (Synthetic Data Test):\n",
      "  ell_max: 8\n",
      "  Test volume shape: (8, 8, 8)\n",
      "  Test projections: 30\n",
      "  Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Using synthetic data for testing\n",
    "# Since the original data file is not available, we'll generate synthetic data\n",
    "\n",
    "# Reconstruction parameters\n",
    "ell_max = 8\n",
    "maxiter = 20\n",
    "regularization_weight = 0 # usually 1.0 \n",
    "use_cuda = True  # Set to False if CUDA is not available\n",
    "\n",
    "# Test parameters\n",
    "num_test_projections = 30  # Number of synthetic projections\n",
    "test_volume_shape = (8, 8, 8)  # Small volume for quick testing\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Configuration (Synthetic Data Test):\")\n",
    "print(f\"  ell_max: {ell_max}\")\n",
    "print(f\"  Test volume shape: {test_volume_shape}\")\n",
    "print(f\"  Test projections: {num_test_projections}\")\n",
    "print(f\"  Using {'CUDA' if use_cuda else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f0a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /myhome/data/smartt/shared/frogbone/dataset_qbin_0009.h5...\n",
      "============================================================\n",
      "INFO:Rotation matrices were loaded from the input file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:227: DeprecationWarning: Entry name rotations is deprecated. Use inner_angle instead.\n",
      "  _deprecated_key_warning('rotations')\n",
      "/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:236: DeprecationWarning: Entry name tilts is deprecated. Use outer_angle instead.\n",
      "  _deprecated_key_warning('tilts')\n",
      "/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:246: DeprecationWarning: Entry name rot_mat is deprecated. Use rotation_matrix instead.\n",
      "  _deprecated_key_warning('rot_mat')\n",
      "/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:268: DeprecationWarning: Entry name offset_j is deprecated. Use j_offset instead.\n",
      "  _deprecated_key_warning('offset_j')\n",
      "/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:278: DeprecationWarning: Entry name offset_k is deprecated. Use k_offset instead.\n",
      "  _deprecated_key_warning('offset_k')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Sample geometry loaded from file.\n",
      "INFO:Detector geometry loaded from file.\n",
      "INFO:Detector geometry loaded from file.\n",
      "\n",
      "Dataset information:\n",
      "  Total projections: 240\n",
      "  Volume shape: [65 82 65]\n",
      "  Projection shape: [ 73 100]\n",
      "\n",
      "Test dataset:\n",
      "  Projections: 10\n",
      "  Volume shape: [65 82 65]\n",
      "\n",
      "Dataset information:\n",
      "  Total projections: 240\n",
      "  Volume shape: [65 82 65]\n",
      "  Projection shape: [ 73 100]\n",
      "\n",
      "Test dataset:\n",
      "  Projections: 10\n",
      "  Volume shape: [65 82 65]\n"
     ]
    }
   ],
   "source": [
    "# Load data from the frogbone dataset\n",
    "data_path = '/myhome/data/smartt/shared/frogbone/dataset_qbin_0009.h5'\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dc_full = DataContainer(data_path, nonfinite_replacement_value=0)\n",
    "total_projections = len(dc_full.projections)\n",
    "\n",
    "print(f\"\\nDataset information:\")\n",
    "print(f\"  Total projections: {total_projections}\")\n",
    "print(f\"  Volume shape: {dc_full.geometry.volume_shape}\")\n",
    "print(f\"  Projection shape: {dc_full.geometry.projection_shape}\")\n",
    "\n",
    "# Create a subset for faster testing - use fewer projections to avoid memory issues\n",
    "test_num_projections = 10\n",
    "dc_test = copy.deepcopy(dc_full)\n",
    "\n",
    "# Delete the full container to free memory\n",
    "del dc_full\n",
    "\n",
    "# Delete projections to keep only first test_num_projections\n",
    "indices_to_delete = list(range(test_num_projections, total_projections))\n",
    "for i in sorted(indices_to_delete, reverse=True):\n",
    "    del dc_test.projections[i]\n",
    "\n",
    "print(f\"\\nTest dataset:\")\n",
    "print(f\"  Projections: {len(dc_test.projections)}\")\n",
    "print(f\"  Volume shape: {dc_test.geometry.volume_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813f969",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Compute Ground Truth\n",
    "\n",
    "We'll compute the ground truth reconstruction using all 240 available projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c753d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data container...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/das/home/barbaf_l/p20639/Mads/frog/frogbone/dataset_qbin_0000.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data container\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data container...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dc_full \u001b[38;5;241m=\u001b[39m \u001b[43mDataContainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonfinite_replacement_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m total_projections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dc_full\u001b[38;5;241m.\u001b[39mprojections)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset information:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:118\u001b[0m, in \u001b[0;36mDataContainer.__init__\u001b[0;34m(self, data_path, data_type, skip_data, nonfinite_replacement_value)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h5_to_projections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown data_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    121\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m load_only_geometry=False.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mumott/data_handling/data_container.py:127\u001b[0m, in \u001b[0;36mDataContainer._h5_to_projections\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_h5_to_projections\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    Internal method for loading data from hdf5 file.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     h5_data \u001b[38;5;241m=\u001b[39m \u001b[43mh5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     projections \u001b[38;5;241m=\u001b[39m h5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojections\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    129\u001b[0m     number_of_projections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(projections)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/das/home/barbaf_l/p20639/Mads/frog/frogbone/dataset_qbin_0000.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load data container\n",
    "print(\"Loading data container...\")\n",
    "dc_full = DataContainer(data_path, nonfinite_replacement_value=0)\n",
    "total_projections = len(dc_full.projections)\n",
    "\n",
    "print(f\"\\nDataset information:\")\n",
    "print(f\"  Total projections available: {total_projections}\")\n",
    "print(f\"  Volume shape: {dc_full.geometry.volume_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c4f20",
   "metadata": {},
   "source": [
    "## 2.1 Test PyTorch Reconstruction\n",
    "\n",
    "Let's test the new PyTorch-accelerated reconstruction and compare it with the LBFGS method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d37f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PyTorch reconstruction...\n",
      "============================================================\n",
      "Test dataset: 10 projections\n",
      "Volume shape: [65 82 65]\n",
      "\n",
      "--- Testing PyTorch AdamW Reconstruction ---\n",
      "Target data shape: torch.Size([10, 73, 100, 8])\n",
      "Volume shape: [65 82 65]\n",
      "Number of SH coefficients: 45\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mumott/methods/basis_sets/spherical_harmonics.py:103: DeprecationWarning: `scipy.special.sph_harm` is deprecated as of SciPy 1.15.0 and will be removed in SciPy 1.17.0. Please use `scipy.special.sph_harm_y` instead.\n",
      "  complex_factors = sph_harm(abs(self._emm_indices)[np.newaxis, np.newaxis, np.newaxis, ...],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH projection matrix shape: torch.Size([10, 8, 45])\n",
      "Built differentiable projector\n",
      "\n",
      "Starting PyTorch optimization (100 iterations)...\n",
      "Regularization: none (weight=0.0)\n",
      "  Iter   10: loss=317045.468750 (data=317045.468750, reg=0.000000)\n",
      "  Iter   10: loss=317045.468750 (data=317045.468750, reg=0.000000)\n",
      "  Iter   20: loss=288696.218750 (data=288696.218750, reg=0.000000)\n",
      "  Iter   20: loss=288696.218750 (data=288696.218750, reg=0.000000)\n",
      "  Iter   30: loss=263326.500000 (data=263326.500000, reg=0.000000)\n",
      "  Iter   30: loss=263326.500000 (data=263326.500000, reg=0.000000)\n",
      "  Iter   40: loss=240478.015625 (data=240478.015625, reg=0.000000)\n",
      "  Iter   40: loss=240478.015625 (data=240478.015625, reg=0.000000)\n",
      "  Iter   50: loss=219952.421875 (data=219952.421875, reg=0.000000)\n",
      "  Iter   50: loss=219952.421875 (data=219952.421875, reg=0.000000)\n",
      "  Iter   60: loss=201604.109375 (data=201604.109375, reg=0.000000)\n",
      "  Iter   60: loss=201604.109375 (data=201604.109375, reg=0.000000)\n",
      "  Iter   70: loss=185313.187500 (data=185313.187500, reg=0.000000)\n",
      "  Iter   70: loss=185313.187500 (data=185313.187500, reg=0.000000)\n",
      "  Iter   80: loss=170969.640625 (data=170969.640625, reg=0.000000)\n",
      "  Iter   80: loss=170969.640625 (data=170969.640625, reg=0.000000)\n",
      "  Iter   90: loss=158461.671875 (data=158461.671875, reg=0.000000)\n",
      "  Iter   90: loss=158461.671875 (data=158461.671875, reg=0.000000)\n",
      "  Iter  100: loss=147670.453125 (data=147670.453125, reg=0.000000)\n",
      "\n",
      "Optimization complete. Final loss: 147670.453125\n",
      "\n",
      "PyTorch reconstruction shape: (65, 82, 65, 45)\n",
      "Reconstruction stats: min=-1.4805, max=1.4745, mean=0.0111\n",
      "  Iter  100: loss=147670.453125 (data=147670.453125, reg=0.000000)\n",
      "\n",
      "Optimization complete. Final loss: 147670.453125\n",
      "\n",
      "PyTorch reconstruction shape: (65, 82, 65, 45)\n",
      "Reconstruction stats: min=-1.4805, max=1.4745, mean=0.0111\n"
     ]
    }
   ],
   "source": [
    "# Test PyTorch reconstruction on the loaded test dataset\n",
    "print(\"Testing PyTorch reconstruction...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Test dataset: {len(dc_test.projections)} projections\")\n",
    "print(f\"Volume shape: {dc_test.geometry.volume_shape}\")\n",
    "\n",
    "# Test PyTorch reconstruction\n",
    "print(\"\\n--- Testing PyTorch AdamW Reconstruction ---\")\n",
    "recon_pytorch = _perform_reconstruction_pytorch(\n",
    "    dc=dc_test,\n",
    "    ell_max=ell_max,\n",
    "    maxiter=100,  # Fewer iterations for test\n",
    "    lr=0.01,\n",
    "    regularization_weight=0.0,\n",
    "    regularization_type='none',\n",
    "    use_cuda=True,\n",
    "    verbose=True,\n",
    "    log_interval=10\n",
    ")\n",
    "print(f\"\\nPyTorch reconstruction shape: {recon_pytorch.shape}\")\n",
    "print(f\"Reconstruction stats: min={recon_pytorch.min():.4f}, max={recon_pytorch.max():.4f}, mean={recon_pytorch.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec06733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Performance Optimization Summary\n",
      "============================================================\n",
      "\n",
      "Key Changes Made:\n",
      "-----------------\n",
      "1. Added cached ASTRA geometries in _AstraMumottOp.__init__()\n",
      "2. Added forward_single() and forward_batched() methods using cached geometry\n",
      "3. Added adjoint_single() and adjoint_batched() methods using cached geometry  \n",
      "4. Added helper functions _backproject_single_gpu/cpu with geometry parameters\n",
      "\n",
      "Performance Results:\n",
      "--------------------\n",
      "Before optimization:\n",
      "  - Forward projection (45 coeffs): ~590ms per iteration\n",
      "  - Backward pass: ~625ms per iteration (estimated)\n",
      "  - Full iteration: ~1.2s\n",
      "\n",
      "After optimization:  \n",
      "  - Forward projection (45 coeffs): ~185-189ms per iteration (3.1x faster)\n",
      "  - Backward pass: ~416ms per iteration (1.5x faster)\n",
      "  - Full iteration: ~410ms (2.9x faster overall)\n",
      "\n",
      "Single ASTRA call timing: ~3.3ms\n",
      "  - Minimum theoretical time for 45 calls: 149ms\n",
      "  - Actual forward time: 185-189ms (27% overhead - acceptable)\n",
      "\n",
      "The remaining bottleneck is the serial nature of ASTRA:\n",
      "  - Each coefficient requires its own ASTRA forward/backward call\n",
      "  - 45 coefficients = 45 serial ASTRA calls\n",
      "  - Cannot be parallelized within a single ASTRA instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance Summary after ASTRA geometry caching optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance Optimization Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The optimization was caching ASTRA geometries in _AstraMumottOp\n",
    "# Before: geometry created for each of 45 coefficients per forward/backward\n",
    "# After: geometry cached once, reused for all 45 coefficients\n",
    "\n",
    "print(\"\"\"\n",
    "Key Changes Made:\n",
    "-----------------\n",
    "1. Added cached ASTRA geometries in _AstraMumottOp.__init__()\n",
    "2. Added forward_single() and forward_batched() methods using cached geometry\n",
    "3. Added adjoint_single() and adjoint_batched() methods using cached geometry  \n",
    "4. Added helper functions _backproject_single_gpu/cpu with geometry parameters\n",
    "\n",
    "Performance Results:\n",
    "--------------------\n",
    "Before optimization:\n",
    "  - Forward projection (45 coeffs): ~590ms per iteration\n",
    "  - Backward pass: ~625ms per iteration (estimated)\n",
    "  - Full iteration: ~1.2s\n",
    "\n",
    "After optimization:  \n",
    "  - Forward projection (45 coeffs): ~185-189ms per iteration (3.1x faster)\n",
    "  - Backward pass: ~416ms per iteration (1.5x faster)\n",
    "  - Full iteration: ~410ms (2.9x faster overall)\n",
    "\n",
    "Single ASTRA call timing: ~3.3ms\n",
    "  - Minimum theoretical time for 45 calls: 149ms\n",
    "  - Actual forward time: 185-189ms (27% overhead - acceptable)\n",
    "\n",
    "The remaining bottleneck is the serial nature of ASTRA:\n",
    "  - Each coefficient requires its own ASTRA forward/backward call\n",
    "  - 45 coefficients = 45 serial ASTRA calls\n",
    "  - Cannot be parallelized within a single ASTRA instance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a137f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Performance Profiling\n",
      "============================================================\n",
      "\n",
      "Test setup:\n",
      "  Volume shape: torch.Size([65, 82, 65, 45])\n",
      "  Projection matrix shape: torch.Size([10, 8, 45])\n",
      "  Device: cuda\n",
      "\n",
      "--- Warm-up run ---\n",
      "\n",
      "--- Test 1: ASTRA Forward Projection ---\n",
      "\n",
      "--- Test 1: ASTRA Forward Projection ---\n",
      "  5 forward projections: 0.926s (185.2ms per iteration)\n",
      "  Output shape: torch.Size([10, 73, 100, 45])\n",
      "\n",
      "--- Test 2: SH Forward (einsum) ---\n",
      "  5 SH forward ops: 0.004s (0.7ms per iteration)\n",
      "  Output shape: torch.Size([10, 73, 100, 8])\n",
      "\n",
      "--- Test 3: Full Forward Pass ---\n",
      "  5 forward projections: 0.926s (185.2ms per iteration)\n",
      "  Output shape: torch.Size([10, 73, 100, 45])\n",
      "\n",
      "--- Test 2: SH Forward (einsum) ---\n",
      "  5 SH forward ops: 0.004s (0.7ms per iteration)\n",
      "  Output shape: torch.Size([10, 73, 100, 8])\n",
      "\n",
      "--- Test 3: Full Forward Pass ---\n",
      "  5 full forward passes: 0.962s (192.3ms per iteration)\n",
      "\n",
      "--- Test 4: Backward Pass ---\n",
      "  5 full forward passes: 0.962s (192.3ms per iteration)\n",
      "\n",
      "--- Test 4: Backward Pass ---\n",
      "  5 forward+backward: 2.080s (415.9ms per iteration)\n",
      "\n",
      "============================================================\n",
      "  5 forward+backward: 2.080s (415.9ms per iteration)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Performance profiling of forward and backward projections\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance Profiling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda')\n",
    "geometry = dc_test.geometry\n",
    "volume_shape = tuple(geometry.volume_shape)\n",
    "num_coeffs = 45\n",
    "\n",
    "# Create test volume\n",
    "test_volume = torch.randn(*volume_shape, num_coeffs, device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Build projector\n",
    "projector = build_mumott_projector(geometry, device=device)\n",
    "\n",
    "# Get projection matrix for SH forward\n",
    "basis_set = SphericalHarmonics(ell_max=8, probed_coordinates=geometry.probed_coordinates)\n",
    "projection_matrix = torch.tensor(basis_set.projection_matrix.astype(np.float32), device=device)\n",
    "\n",
    "print(f\"\\nTest setup:\")\n",
    "print(f\"  Volume shape: {test_volume.shape}\")\n",
    "print(f\"  Projection matrix shape: {projection_matrix.shape}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Warm up\n",
    "print(\"\\n--- Warm-up run ---\")\n",
    "_ = projector(test_volume)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Test 1: Forward projection only (ASTRA)\n",
    "print(\"\\n--- Test 1: ASTRA Forward Projection ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    spatial_proj = projector(test_volume)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(f\"  5 forward projections: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")\n",
    "print(f\"  Output shape: {spatial_proj.shape}\")\n",
    "\n",
    "# Test 2: SH Forward (einsum)\n",
    "print(\"\\n--- Test 2: SH Forward (einsum) ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    predicted = forward_quadrature(\n",
    "        geometry.probed_coordinates,\n",
    "        spatial_proj,\n",
    "        ell_max=8,\n",
    "        projection_matrix=projection_matrix\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(f\"  5 SH forward ops: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")\n",
    "print(f\"  Output shape: {predicted.shape}\")\n",
    "\n",
    "# Test 3: Full forward pass\n",
    "print(\"\\n--- Test 3: Full Forward Pass ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    spatial_proj = projector(test_volume)\n",
    "    predicted = forward_quadrature(\n",
    "        geometry.probed_coordinates,\n",
    "        spatial_proj,\n",
    "        ell_max=8,\n",
    "        projection_matrix=projection_matrix\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(f\"  5 full forward passes: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")\n",
    "\n",
    "# Test 4: Backward pass\n",
    "print(\"\\n--- Test 4: Backward Pass ---\")\n",
    "target = torch.randn_like(predicted)\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    test_volume.grad = None\n",
    "    spatial_proj = projector(test_volume)\n",
    "    predicted = forward_quadrature(\n",
    "        geometry.probed_coordinates,\n",
    "        spatial_proj,\n",
    "        ell_max=8,\n",
    "        projection_matrix=projection_matrix\n",
    "    )\n",
    "    loss = torch.nn.functional.mse_loss(predicted, target)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(f\"  5 forward+backward: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28356abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating the batching issue...\n",
      "test_volume.ndim = 4\n",
      "test_volume.shape = torch.Size([65, 82, 65, 45])\n",
      "Last dimension (coefficients) = 45\n",
      "\n",
      "The projector sees this as 45 BATCHES, calling ASTRA 45 times!\n",
      "This is the bottleneck - we need to vectorize over coefficients.\n"
     ]
    }
   ],
   "source": [
    "# Check what's happening with batching\n",
    "print(\"Investigating the batching issue...\")\n",
    "print(f\"test_volume.ndim = {test_volume.ndim}\")\n",
    "print(f\"test_volume.shape = {test_volume.shape}\")\n",
    "print(f\"Last dimension (coefficients) = {test_volume.shape[-1]}\")\n",
    "print(\"\\nThe projector sees this as 45 BATCHES, calling ASTRA 45 times!\")\n",
    "print(\"This is the bottleneck - we need to vectorize over coefficients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2ff2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module reloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Reload the projector module after changes\n",
    "import importlib\n",
    "import smartt.projectors.astra_projector\n",
    "importlib.reload(smartt.projectors.astra_projector)\n",
    "from smartt.projectors.astra_projector import forward_project, build_mumott_projector\n",
    "\n",
    "print(\"Module reloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0993bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Single vs Batched Projection Timing\n",
      "============================================================\n",
      "\n",
      "--- Single Volume (1 ASTRA call) ---\n",
      "  10 single projections: 0.033s (3.3ms per projection)\n",
      "\n",
      "--- Batched Volume (45 ASTRA calls, shape torch.Size([65, 82, 65, 45])) ---\n",
      "  5 batched projections: 0.944s (188.9ms per batch)\n",
      "\n",
      "--- Analysis ---\n",
      "  Time per single ASTRA call: 3.3ms\n",
      "  Time per batched call (45 ASTRA): 188.9ms\n",
      "  Overhead per batch: 39.7ms (should be ~0)\n",
      "  Expected time if 45 calls: 149.1ms\n",
      "  Actual overhead factor: 1.27x\n"
     ]
    }
   ],
   "source": [
    "# Compare single vs batched projection performance\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Single vs Batched Projection Timing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Single 3D volume (no batch dimension)\n",
    "test_single = torch.randn(*volume_shape, device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Build projector (this caches geometries)\n",
    "projector = build_mumott_projector(geometry, device=device)\n",
    "\n",
    "# Warm up\n",
    "_ = projector(test_single)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Test single volume (1 ASTRA call)\n",
    "print(\"\\n--- Single Volume (1 ASTRA call) ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    _ = projector(test_single)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "single_time = (end - start) / 10\n",
    "print(f\"  10 single projections: {end - start:.3f}s ({single_time*1000:.1f}ms per projection)\")\n",
    "\n",
    "# Test batched volume (45 ASTRA calls)\n",
    "test_batched = torch.randn(*volume_shape, 45, device=device, dtype=torch.float32, requires_grad=True)\n",
    "print(f\"\\n--- Batched Volume (45 ASTRA calls, shape {test_batched.shape}) ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    _ = projector(test_batched)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "batched_time = (end - start) / 5\n",
    "print(f\"  5 batched projections: {end - start:.3f}s ({batched_time*1000:.1f}ms per batch)\")\n",
    "\n",
    "print(f\"\\n--- Analysis ---\")\n",
    "print(f\"  Time per single ASTRA call: {single_time*1000:.1f}ms\")\n",
    "print(f\"  Time per batched call (45 ASTRA): {batched_time*1000:.1f}ms\")\n",
    "print(f\"  Overhead per batch: {batched_time*1000 - 45*single_time*1000:.1f}ms (should be ~0)\")\n",
    "print(f\"  Expected time if 45 calls: {45*single_time*1000:.1f}ms\")\n",
    "print(f\"  Actual overhead factor: {batched_time / (45*single_time):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cef559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "mumott SAXSProjectorCUDA Comparison\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Let's also check what mumott's native SAXSProjectorCUDA does\n",
    "from mumott.methods.projectors import SAXSProjectorCUDA\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"mumott SAXSProjectorCUDA Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create SAXSProjectorCUDA\n",
    "saxs_proj = SAXSProjectorCUDA(geometry)\n",
    "\n",
    "# Create volume in mumott format \n",
    "# mumott expects: (nx, ny, nz, n_coeffs)\n",
    "vol_mumott = np.random.randn(*volume_shape, 45).astype(np.float32)\n",
    "print(f\"mumott volume shape: {vol_mumott.shape}\")\n",
    "print(f\"geometry.volume_shape: {geometry.volume_shape}\")\n",
    "\n",
    "# Warm up\n",
    "_ = saxs_proj.forward(vol_mumott)\n",
    "\n",
    "# Time mumott's forward\n",
    "print(\"\\n--- mumott SAXSProjectorCUDA.forward ---\")\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    result = saxs_proj.forward(vol_mumott)\n",
    "end = time.time()\n",
    "print(f\"  5 forward projections: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")\n",
    "print(f\"  Output shape: {result.shape}\")\n",
    "\n",
    "# Compare to our implementation\n",
    "print(\"\\n--- Our build_mumott_projector ---\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    _ = projector(test_batched)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(f\"  5 forward projections: {end - start:.3f}s ({(end-start)/5*1000:.1f}ms per iteration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ground truth reconstruction with all projections\n",
    "print(f\"\\nComputing ground truth reconstruction with all {total_projections} projections...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "ground_truth = _perform_reconstruction(\n",
    "    dc=dc_full,\n",
    "    ell_max=ell_max,\n",
    "    maxiter=maxiter,\n",
    "    regularization_weight=regularization_weight,\n",
    "    use_cuda=use_cuda,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGround truth shape: {ground_truth.shape}\")\n",
    "volume_shape = ground_truth.shape[:3]\n",
    "num_coeffs = ground_truth.shape[3]\n",
    "print(f\"Volume shape: {volume_shape}\")\n",
    "print(f\"Number of coefficients: {num_coeffs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fee72f",
   "metadata": {},
   "source": [
    "## 3. Select Fixed Sparse Subset\n",
    "\n",
    "Randomly select 60 projections from the full set. This subset will remain fixed for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd22a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select fixed sparse subset of 60 projections\n",
    "np.random.seed(seed)\n",
    "fixed_sparse_indices = np.random.choice(\n",
    "    total_projections,\n",
    "    size=num_projections_sparse,\n",
    "    replace=False\n",
    ")\n",
    "fixed_sparse_indices = np.arange(num_projections_sparse)\n",
    "\n",
    "fixed_sparse_indices = np.sort(fixed_sparse_indices)\n",
    "\n",
    "fixed_sparse_indices = np.linspace(0, 50, num_projections_sparse, dtype=int)\n",
    "\n",
    "print(f\"Fixed sparse subset (60 projections):\")\n",
    "print(f\"  Indices: {fixed_sparse_indices[:10]}...{fixed_sparse_indices[-10:]}\")\n",
    "print(f\"  Min: {fixed_sparse_indices.min()}, Max: {fixed_sparse_indices.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7c650",
   "metadata": {},
   "source": [
    "## 4. Run Multiple Sparse Reconstruction Experiments\n",
    "\n",
    "For each experiment:\n",
    "1. Randomly sample 80% of the fixed 60 projections (48 projections)\n",
    "2. Perform sparse reconstruction with these 48 projections\n",
    "3. Store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4be673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate array to store all reconstructions\n",
    "all_reconstructions = np.zeros(\n",
    "    (num_experiments, *volume_shape, num_coeffs),\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "# Store the indices used in each experiment\n",
    "subsample_indices_list = []\n",
    "\n",
    "print(f\"\\nRunning {num_experiments} sparse reconstruction experiments...\")\n",
    "print(f\"Each experiment uses {num_subsamples} randomly sampled projections from the fixed subset of 60.\\n\")\n",
    "\n",
    "for exp_idx in tqdm(range(num_experiments), desc=\"Experiments\"):\n",
    "    # Randomly sample 80% of the fixed sparse subset\n",
    "    subsample_indices = np.random.choice(\n",
    "        fixed_sparse_indices,\n",
    "        size=num_subsamples,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    subsample_indices = np.sort(subsample_indices)\n",
    "    subsample_indices_list.append(subsample_indices)\n",
    "    \n",
    "    # Create a fresh data container with only the subsampled projections\n",
    "    dc_subsample = DataContainer(data_path, nonfinite_replacement_value=0)\n",
    "    \n",
    "    # Remove projections not in the subsample\n",
    "    all_indices = np.arange(total_projections)\n",
    "    indices_to_delete = [i for i in all_indices if i not in subsample_indices]\n",
    "    \n",
    "    for i in sorted(indices_to_delete, reverse=True):\n",
    "        del dc_subsample.projections[i]\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    reconstruction = _perform_reconstruction(\n",
    "        dc=dc_subsample,\n",
    "        ell_max=ell_max,\n",
    "        maxiter=maxiter,\n",
    "        regularization_weight=regularization_weight,\n",
    "        use_cuda=use_cuda,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Store reconstruction\n",
    "    all_reconstructions[exp_idx] = reconstruction\n",
    "    \n",
    "    # Clean up\n",
    "    del dc_subsample, reconstruction\n",
    "\n",
    "print(f\"\\nCompleted {num_experiments} experiments.\")\n",
    "print(f\"All reconstructions shape: {all_reconstructions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59770d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 10 slices of all_reconstructions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    if i < num_experiments:\n",
    "        slice_data = all_reconstructions[i, 32, :, :, 0]\n",
    "        im = axes[i].imshow(slice_data, cmap='viridis')\n",
    "        axes[i].set_title(f'Experiment {i}', fontsize=11, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[i].axis('off')\n",
    "\n",
    "fig.suptitle('First 10 Reconstructions at z=32, coefficient 0', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db19b5c",
   "metadata": {},
   "source": [
    "## 5. Compute Standard Deviation Across Experiments\n",
    "\n",
    "Calculate the standard deviation across all experiments to quantify reconstruction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7867910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation across experiments\n",
    "mean_reconstruction = np.mean(all_reconstructions, axis=0)\n",
    "std_reconstruction = np.std(all_reconstructions, axis=0)\n",
    "\n",
    "print(f\"Mean reconstruction shape: {mean_reconstruction.shape}\")\n",
    "print(f\"Std reconstruction shape: {std_reconstruction.shape}\")\n",
    "\n",
    "# Compute statistics\n",
    "print(f\"\\nStandard deviation statistics (across all voxels and coefficients):\")\n",
    "print(f\"  Mean std: {np.mean(std_reconstruction):.6f}\")\n",
    "print(f\"  Median std: {np.median(std_reconstruction):.6f}\")\n",
    "print(f\"  Min std: {np.min(std_reconstruction):.6f}\")\n",
    "print(f\"  Max std: {np.max(std_reconstruction):.6f}\")\n",
    "\n",
    "# Per-coefficient statistics\n",
    "print(f\"\\nPer-coefficient std statistics:\")\n",
    "for coeff_idx in range(num_coeffs):\n",
    "    coeff_std = std_reconstruction[..., coeff_idx]\n",
    "    print(f\"  Coefficient {coeff_idx:2d}: mean={np.mean(coeff_std):.6f}, \"\n",
    "          f\"median={np.median(coeff_std):.6f}, max={np.max(coeff_std):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc6afe",
   "metadata": {},
   "source": [
    "## 6. Visualize Standard Deviation Slices\n",
    "\n",
    "Plot slices through the 3D standard deviation volume for different spherical harmonic coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf699774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select coefficients to visualize\n",
    "# Coefficient 0 is l=0 (isotropic), then l=2, l=4, l=6, l=8\n",
    "coeffs_to_plot = [0, 1, 6, 15, 28, 44]  # Start of each l value: l=0, l=2, l=4, l=6, l=8, last coeff\n",
    "coeff_labels = ['=0 (iso)', '=2, m=-2', '=4, m=-4', '=6, m=-6', '=8, m=-8', '=8, m=8']\n",
    "\n",
    "# Select middle slice\n",
    "mid_z = volume_shape[2] // 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (coeff_idx, label) in enumerate(zip(coeffs_to_plot, coeff_labels)):\n",
    "    std_slice = std_reconstruction[:, :, mid_z, coeff_idx]\n",
    "    \n",
    "    im = axes[idx].imshow(std_slice.T, cmap='hot', origin='lower')\n",
    "    axes[idx].set_title(f'Std Dev - {label}\\n(z={mid_z})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Standard Deviation Across {num_experiments} Reconstructions\\n'\n",
    "             f'(Each with {num_subsamples} random projections from fixed subset of {num_projections_sparse})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d37d2",
   "metadata": {},
   "source": [
    "## 7. Multiple Z-Slices for Selected Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple z-slices for the isotropic component (l=0)\n",
    "coeff_idx = 1\n",
    "z_slices = [volume_shape[2]//4, volume_shape[2]//2, 3*volume_shape[2]//4]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, z in enumerate(z_slices):\n",
    "    std_slice = std_reconstruction[:, :, z, coeff_idx]\n",
    "    \n",
    "    im = axes[idx].imshow(std_slice.T, cmap='hot', origin='lower')\n",
    "    axes[idx].set_title(f'Std Dev - =0 (isotropic)\\nz={z}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Standard Deviation Across Different Z-Slices', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f120830",
   "metadata": {},
   "source": [
    "## 8. Compare Mean Reconstruction vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean of sparse reconstructions to ground truth\n",
    "coeff_idx = 2  # Isotropic component\n",
    "mid_z = volume_shape[2] // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Determine shared colorbar limits\n",
    "vmin = min(mean_reconstruction[:, :, mid_z, coeff_idx].min(),\n",
    "           ground_truth[:, :, mid_z, coeff_idx].min())\n",
    "vmax = max(mean_reconstruction[:, :, mid_z, coeff_idx].max(),\n",
    "           ground_truth[:, :, mid_z, coeff_idx].max())\n",
    "\n",
    "# Mean reconstruction\n",
    "im0 = axes[0].imshow(mean_reconstruction[:, :, mid_z, coeff_idx].T,\n",
    "                     cmap='viridis', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[0].set_title(f'Mean of {num_experiments} Sparse\\nReconstructions (=0)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Ground truth\n",
    "im1 = axes[1].imshow(ground_truth[:, :, mid_z, coeff_idx].T,\n",
    "                     cmap='viridis', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[1].set_title(f'Ground Truth\\n(All {total_projections} projections, =0)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Difference\n",
    "diff = mean_reconstruction[:, :, mid_z, coeff_idx] - ground_truth[:, :, mid_z, coeff_idx]\n",
    "im2 = axes[2].imshow(diff.T, cmap='RdBu_r', origin='lower',\n",
    "                     vmin=-np.abs(diff).max(), vmax=np.abs(diff).max())\n",
    "axes[2].set_title('Difference\\n(Mean - Ground Truth)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Mean Sparse Reconstruction vs Ground Truth (z={mid_z})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute error metrics\n",
    "mse = np.mean((mean_reconstruction - ground_truth)**2)\n",
    "mae = np.mean(np.abs(mean_reconstruction - ground_truth))\n",
    "print(f\"\\nError metrics (Mean reconstruction vs Ground truth):\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  MAE: {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c48af",
   "metadata": {},
   "source": [
    "## 9. Histogram of Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89badf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of standard deviations for different coefficients\n",
    "coeffs_to_plot = [0, 1, 6, 15, 28, 44]\n",
    "coeff_labels = ['=0', '=2, m=-2', '=4, m=-4', '=6, m=-6', '=8, m=-8', '=8, m=8']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (coeff_idx, label) in enumerate(zip(coeffs_to_plot, coeff_labels)):\n",
    "    std_flat = std_reconstruction[..., coeff_idx].flatten()\n",
    "    \n",
    "    axes[idx].hist(std_flat, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xlabel('Standard Deviation', fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].set_title(f'{label}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_std = np.mean(std_flat)\n",
    "    median_std = np.median(std_flat)\n",
    "    axes[idx].axvline(mean_std, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_std:.4f}')\n",
    "    axes[idx].axvline(median_std, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_std:.4f}')\n",
    "    axes[idx].legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(f'Distribution of Standard Deviations Across Voxels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bed6a",
   "metadata": {},
   "source": [
    "## 10. Coefficient-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e92767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean std for each coefficient\n",
    "mean_std_per_coeff = np.array([np.mean(std_reconstruction[..., i]) for i in range(num_coeffs)])\n",
    "median_std_per_coeff = np.array([np.median(std_reconstruction[..., i]) for i in range(num_coeffs)])\n",
    "max_std_per_coeff = np.array([np.max(std_reconstruction[..., i]) for i in range(num_coeffs)])\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "x = np.arange(num_coeffs)\n",
    "\n",
    "# Mean std per coefficient\n",
    "axes[0].bar(x, mean_std_per_coeff, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Coefficient Index', fontsize=11)\n",
    "axes[0].set_ylabel('Mean Std Dev', fontsize=11)\n",
    "axes[0].set_title('Mean Standard Deviation per Coefficient', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Median std per coefficient\n",
    "axes[1].bar(x, median_std_per_coeff, color='orange', alpha=0.7)\n",
    "axes[1].set_xlabel('Coefficient Index', fontsize=11)\n",
    "axes[1].set_ylabel('Median Std Dev', fontsize=11)\n",
    "axes[1].set_title('Median Standard Deviation per Coefficient', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Max std per coefficient\n",
    "axes[2].bar(x, max_std_per_coeff, color='crimson', alpha=0.7)\n",
    "axes[2].set_xlabel('Coefficient Index', fontsize=11)\n",
    "axes[2].set_ylabel('Max Std Dev', fontsize=11)\n",
    "axes[2].set_title('Maximum Standard Deviation per Coefficient', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add  value labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([0, 1, 6, 15, 28])\n",
    "    ax.set_xticklabels(['=0', '=2', '=4', '=6', '=8'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde2cb4",
   "metadata": {},
   "source": [
    "## 11. Relative Uncertainty\n",
    "\n",
    "Compute relative uncertainty (std / mean) to understand uncertainty relative to signal strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d927eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relative uncertainty (coefficient of variation)\n",
    "# Add small epsilon to avoid division by zero\n",
    "epsilon = 1e-10\n",
    "relative_uncertainty = std_reconstruction / (np.abs(mean_reconstruction) + epsilon)\n",
    "\n",
    "# Clip extreme values for visualization\n",
    "relative_uncertainty_clipped = np.clip(relative_uncertainty, 0, 5)\n",
    "\n",
    "print(f\"Relative uncertainty statistics:\")\n",
    "print(f\"  Mean: {np.mean(relative_uncertainty):.4f}\")\n",
    "print(f\"  Median: {np.median(relative_uncertainty):.4f}\")\n",
    "print(f\"  90th percentile: {np.percentile(relative_uncertainty, 90):.4f}\")\n",
    "print(f\"  99th percentile: {np.percentile(relative_uncertainty, 99):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f492f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relative uncertainty for selected coefficients\n",
    "coeffs_to_plot = [0, 1, 6, 15, 28, 44]\n",
    "coeff_labels = ['=0', '=2, m=-2', '=4, m=-4', '=6, m=-6', '=8, m=-8', '=8, m=8']\n",
    "mid_z = volume_shape[2] // 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (coeff_idx, label) in enumerate(zip(coeffs_to_plot, coeff_labels)):\n",
    "    rel_unc_slice = relative_uncertainty_clipped[:, :, mid_z, coeff_idx]\n",
    "    \n",
    "    im = axes[idx].imshow(rel_unc_slice.T, cmap='plasma', origin='lower', vmin=0, vmax=2)\n",
    "    axes[idx].set_title(f'Relative Uncertainty - {label}\\n(z={mid_z})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Relative Uncertainty (Std / |Mean|) - Clipped to [0, 2]', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf830c1",
   "metadata": {},
   "source": [
    "## 12. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168865e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save results to HDF5\n",
    "save_results = True  # Set to True to save\n",
    "output_path = '/myhome/smartt/results/sparse_uncertainty_analysis.h5'\n",
    "\n",
    "if save_results:\n",
    "    print(f\"Saving results to {output_path}...\")\n",
    "    \n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        # Save reconstructions\n",
    "        f.create_dataset('all_reconstructions', data=all_reconstructions, compression='gzip')\n",
    "        f.create_dataset('mean_reconstruction', data=mean_reconstruction, compression='gzip')\n",
    "        f.create_dataset('std_reconstruction', data=std_reconstruction, compression='gzip')\n",
    "        f.create_dataset('ground_truth', data=ground_truth, compression='gzip')\n",
    "        \n",
    "        # Save indices\n",
    "        f.create_dataset('fixed_sparse_indices', data=fixed_sparse_indices)\n",
    "        for i, indices in enumerate(subsample_indices_list):\n",
    "            f.create_dataset(f'subsample_indices_{i}', data=indices)\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['ell_max'] = ell_max\n",
    "        f.attrs['num_experiments'] = num_experiments\n",
    "        f.attrs['num_projections_sparse'] = num_projections_sparse\n",
    "        f.attrs['num_subsamples'] = num_subsamples\n",
    "        f.attrs['total_projections'] = total_projections\n",
    "        f.attrs['volume_shape'] = volume_shape\n",
    "        f.attrs['num_coeffs'] = num_coeffs\n",
    "        f.attrs['seed'] = seed\n",
    "    \n",
    "    print(f\"Results saved successfully!\")\n",
    "else:\n",
    "    print(\"Results not saved (set save_results=True to save)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results from HDF5\n",
    "load_results = False  # Set to True to load\n",
    "input_path = '/myhome/smartt/results/sparse_uncertainty_analysis.h5'\n",
    "\n",
    "if load_results:\n",
    "    print(f\"Loading results from {input_path}...\")\n",
    "    \n",
    "    with h5py.File(input_path, 'r') as f:\n",
    "        # Load reconstructions\n",
    "        all_reconstructions = f['all_reconstructions'][:]\n",
    "        mean_reconstruction = f['mean_reconstruction'][:]\n",
    "        std_reconstruction = f['std_reconstruction'][:]\n",
    "        ground_truth = f['ground_truth'][:]\n",
    "        \n",
    "        # Load indices\n",
    "        fixed_sparse_indices = f['fixed_sparse_indices'][:]\n",
    "        subsample_indices_list = []\n",
    "        for i in range(f.attrs['num_experiments']):\n",
    "            subsample_indices = f[f'subsample_indices_{i}'][:]\n",
    "            subsample_indices_list.append(subsample_indices)\n",
    "        \n",
    "        # Load metadata\n",
    "        ell_max = f.attrs['ell_max']\n",
    "        num_experiments = f.attrs['num_experiments']\n",
    "        num_projections_sparse = f.attrs['num_projections_sparse']\n",
    "        num_subsamples = f.attrs['num_subsamples']\n",
    "        total_projections = f.attrs['total_projections']\n",
    "        volume_shape = tuple(f.attrs['volume_shape'])\n",
    "        num_coeffs = f.attrs['num_coeffs']\n",
    "        seed = f.attrs['seed']\n",
    "    \n",
    "    print(f\"Results loaded successfully!\")\n",
    "    print(f\"\\nLoaded data:\")\n",
    "    print(f\"  all_reconstructions shape: {all_reconstructions.shape}\")\n",
    "    print(f\"  mean_reconstruction shape: {mean_reconstruction.shape}\")\n",
    "    print(f\"  std_reconstruction shape: {std_reconstruction.shape}\")\n",
    "    print(f\"  ground_truth shape: {ground_truth.shape}\")\n",
    "    print(f\"  Number of experiments: {num_experiments}\")\n",
    "    print(f\"  Fixed sparse subset: {len(fixed_sparse_indices)} projections\")\n",
    "    print(f\"  Subsample size: {num_subsamples} projections\")\n",
    "else:\n",
    "    print(\"Results not loaded (set load_results=True to load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f428f5b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Ground Truth**: Computed using all 240 available projections\n",
    "2. **Fixed Sparse Subset**: 60 projections randomly selected and fixed\n",
    "3. **Multiple Experiments**: 20 reconstructions, each using 48 randomly sampled projections from the fixed subset\n",
    "4. **Uncertainty Quantification**: Standard deviation across experiments shows reconstruction uncertainty\n",
    "5. **Coefficient Analysis**: Different spherical harmonic coefficients show varying levels of uncertainty\n",
    "6. **Relative Uncertainty**: Normalized by signal strength to identify problematic regions\n",
    "\n",
    "Key findings:\n",
    "- Higher-order coefficients (larger  values) typically show higher uncertainty\n",
    "- Regions with low signal show higher relative uncertainty\n",
    "- The standard deviation provides a measure of reconstruction stability given sparse sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3ac1b",
   "metadata": {},
   "source": [
    "## 13. Forward Project Reconstructions to Analyze Angular Variability\n",
    "\n",
    "We'll project each reconstruction through all projection angles and analyze which angles show the highest variability across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_vecs = dc_full.geometry.probed_coordinates.vector[69:75]\n",
    "points3D = torch.tensor(subset_vecs).reshape(-1, 3)\n",
    "points3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "colors = torch.arange(len(subset_vecs)).repeat_interleave(24)\n",
    "colors = colors.reshape(-1)\n",
    "colors, torch.tensor(points3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be03248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create 3D plot of probed coordinates\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create sphere surface\n",
    "u = np.linspace(0, 2 * np.pi, 50)\n",
    "v = np.linspace(0, np.pi, 50)\n",
    "x_sphere = np.outer(np.cos(u), np.sin(v))\n",
    "y_sphere = np.outer(np.sin(u), np.sin(v))\n",
    "z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "# Plot translucent sphere\n",
    "ax.plot_surface(x_sphere, y_sphere, z_sphere, color='cyan', alpha=0.1, edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Translucent Sphere (Radius = 1)', fontsize=14, fontweight='bold')\n",
    "ax.set_box_aspect([1,1,1])\n",
    "\n",
    "# Plot the 3D points\n",
    "ax.scatter(points3D[:, 0], points3D[:, 1], points3D[:, 2], \n",
    "           c=colors, s=10, cmap='viridis', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_zlabel('Z', fontsize=11)\n",
    "ax.set_title('Probed Coordinates in 3D Space', fontsize=14, fontweight='bold')\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total number of probed points: {len(points3D)}\")\n",
    "print(f\"Points shape: {points3D.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c72b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create projector using the full geometry from dc_full\n",
    "print(\"Creating projector with full geometry...\")\n",
    "projector_full = SAXSProjector(dc_full.geometry)\n",
    "\n",
    "# Create basis set for forward projection\n",
    "basis_set = SphericalHarmonics(ell_max=ell_max, probed_coordinates=dc_full.geometry.probed_coordinates)\n",
    "\n",
    "print(f\"Projector geometry:\")\n",
    "print(f\"  Number of projections: {len(dc_full.geometry.inner_angles)}\")\n",
    "print(f\"  Projection shape: {dc_full.geometry.projection_shape}\")\n",
    "print(f\"  Detector angles: {dc_full.geometry.detector_angles.shape}\")\n",
    "\n",
    "num_angles = len(dc_full.geometry.inner_angles)\n",
    "proj_shape = dc_full.geometry.projection_shape\n",
    "num_detector_angles = len(dc_full.geometry.detector_angles)\n",
    "\n",
    "print(f\"\\nExpected output shape per reconstruction: ({num_angles}, {proj_shape[0]}, {proj_shape[1]}, {num_detector_angles})\")\n",
    "print(f\"Expected output shape for all reconstructions: ({num_experiments}, {num_angles}, {proj_shape[0]}, {proj_shape[1]}, {num_detector_angles})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db947b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward project all reconstructions\n",
    "print(f\"\\nForward projecting {num_experiments} reconstructions through {num_angles} angles...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "# Pre-allocate array for all forward projections\n",
    "all_forward_projections = np.zeros(\n",
    "    (num_experiments, num_angles, proj_shape[0], proj_shape[1], num_detector_angles),\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "for exp_idx in tqdm(range(num_experiments), desc=\"Forward projecting\"):\n",
    "    reconstruction = all_reconstructions[exp_idx].astype(np.float64)\n",
    "    \n",
    "    # Forward project: projector.forward() then basis_set.forward()\n",
    "    spatial_projection = projector_full.forward(reconstruction)\n",
    "    forward_proj = basis_set.forward(spatial_projection)\n",
    "    \n",
    "    all_forward_projections[exp_idx] = forward_proj.astype(np.float32)\n",
    "    \n",
    "    # Clean up\n",
    "    del reconstruction, spatial_projection, forward_proj\n",
    "\n",
    "print(f\"\\nForward projections shape: {all_forward_projections.shape}\")\n",
    "print(f\"Expected shape: ({num_experiments}, {num_angles}, {proj_shape[0]}, {proj_shape[1]}, {num_detector_angles})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard deviation across experiments for each angle\n",
    "print(\"Computing variability across experiments for each projection angle...\")\n",
    "\n",
    "# Compute std across experiments (axis 0) for each angle\n",
    "# Result shape: (num_angles, proj_shape[0], proj_shape[1], num_detector_angles)\n",
    "std_per_angle = np.std(all_forward_projections, axis=0)\n",
    "\n",
    "# Compute total variability per angle (sum of variance across all pixels and detector angles)\n",
    "variability_per_angle = np.sum(std_per_angle**2, axis=(1, 2, 3))\n",
    "\n",
    "print(f\"\\nStandard deviation per angle shape: {std_per_angle.shape}\")\n",
    "print(f\"Variability per angle shape: {variability_per_angle.shape}\")\n",
    "print(f\"\\nVariability statistics:\")\n",
    "print(f\"  Mean: {np.mean(variability_per_angle):.4f}\")\n",
    "print(f\"  Median: {np.median(variability_per_angle):.4f}\")\n",
    "print(f\"  Min: {np.min(variability_per_angle):.4f}\")\n",
    "print(f\"  Max: {np.max(variability_per_angle):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find angles with highest and lowest variability\n",
    "num_top_angles = 20\n",
    "\n",
    "# Get indices sorted by variability\n",
    "sorted_indices = np.argsort(variability_per_angle)[::-1]  # Descending order\n",
    "\n",
    "# Top 20 most variable angles\n",
    "top_variable_indices = sorted_indices[:num_top_angles]\n",
    "top_variable_values = variability_per_angle[top_variable_indices]\n",
    "\n",
    "# Bottom 20 least variable angles\n",
    "least_variable_indices = sorted_indices[-num_top_angles:]\n",
    "least_variable_values = variability_per_angle[least_variable_indices]\n",
    "\n",
    "fixed_sparse_values = variability_per_angle[fixed_sparse_indices]\n",
    "\n",
    "print(f\"Top {num_top_angles} most variable angles:\")\n",
    "for i, (idx, val) in enumerate(zip(top_variable_indices, top_variable_values)):\n",
    "    print(f\"  {i+1}. Angle {idx}: variability = {val:.4f}\")\n",
    "\n",
    "print(f\"\\nTop {num_top_angles} least variable angles:\")\n",
    "for i, (idx, val) in enumerate(zip(least_variable_indices, least_variable_values)):\n",
    "    print(f\"  {i+1}. Angle {idx}: variability = {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5238d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variability as a function of angle index\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(variability_per_angle, 'o-', linewidth=1, markersize=3, alpha=0.7)\n",
    "ax.scatter(top_variable_indices, top_variable_values, color='red', s=50, \n",
    "           label=f'Top {num_top_angles} most variable', zorder=5)\n",
    "ax.scatter(least_variable_indices, least_variable_values, color='green', s=50,\n",
    "           label=f'Top {num_top_angles} least variable', zorder=5)\n",
    "\n",
    "ax.scatter(fixed_sparse_indices, fixed_sparse_values, color='blue', s=50,\n",
    "           label=f'Fixed Sparse Angles', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Projection Angle Index', fontsize=12)\n",
    "ax.set_ylabel('Total Variability (Sum of Variance)', fontsize=12)\n",
    "ax.set_title('Variability Across Projection Angles', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a875ac",
   "metadata": {},
   "source": [
    "### 3D Visualization of High-Variability Angles\n",
    "\n",
    "Visualize the projection angles with the highest variability on a 3D sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad70de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rotation and tilt angles from geometry\n",
    "rotation_angles_rad = dc_full.geometry.inner_angles[:]\n",
    "tilt_angles_rad = list(np.pi/2 - np.array(dc_full.geometry.outer_angles))[:]\n",
    "\n",
    "# Convert to Cartesian coordinates on unit sphere\n",
    "r = 1\n",
    "x_all = r * np.sin(tilt_angles_rad) * np.cos(rotation_angles_rad)\n",
    "y_all = r * np.sin(tilt_angles_rad) * np.sin(rotation_angles_rad)\n",
    "z_all = r * np.cos(tilt_angles_rad)\n",
    "\n",
    "# Extract coordinates for high-variability angles\n",
    "x_high_var = x_all[top_variable_indices]\n",
    "y_high_var = y_all[top_variable_indices]\n",
    "z_high_var = z_all[top_variable_indices]\n",
    "\n",
    "print(f\"Plotted {len(x_all)} total angles\")\n",
    "print(f\"Highlighted {len(x_high_var)} high-variability angles in red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D sphere plot showing all angles and highlighting high-variability ones\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create sphere surface\n",
    "u = np.linspace(0, 2 * np.pi, 100)\n",
    "v = np.linspace(0, np.pi, 100)\n",
    "X = r * np.outer(np.cos(u), np.sin(v))\n",
    "Y = r * np.outer(np.sin(u), np.sin(v))\n",
    "Z = r * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "# Plot sphere (semi-transparent)\n",
    "ax.plot_surface(X, Y, Z, color='lightblue', alpha=0.1)\n",
    "\n",
    "# Plot all projection angles (gray)\n",
    "ax.scatter(x_all, y_all, z_all, color='gray', s=20, alpha=0.5, label='All angles')\n",
    "\n",
    "# Highlight high-variability angles (red)\n",
    "ax.scatter(x_high_var, y_high_var, z_high_var, color='red', s=100, \n",
    "           alpha=0.9, edgecolors='darkred', linewidths=2,\n",
    "           label=f'Top {num_top_angles} highest variability')\n",
    "\n",
    "ax.set_xlabel('X', fontsize=10)\n",
    "ax.set_ylabel('Y', fontsize=10)\n",
    "ax.set_zlabel('Z', fontsize=10)\n",
    "ax.set_title('Projection Angles: High-Variability Highlighted', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.view_init(elev=10, azim=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a020a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fixed sparse subset vs high-variability angles in 3D\n",
    "# Extract coordinates for fixed sparse subset angles\n",
    "x_sparse = x_all[fixed_sparse_indices]\n",
    "y_sparse = y_all[fixed_sparse_indices]\n",
    "z_sparse = z_all[fixed_sparse_indices]\n",
    "\n",
    "print(f\"Fixed sparse subset: {len(fixed_sparse_indices)} angles\")\n",
    "print(f\"Top high-variability: {len(top_variable_indices)} angles\")\n",
    "\n",
    "# Check overlap between fixed sparse subset and high-variability angles\n",
    "overlap = np.intersect1d(fixed_sparse_indices, top_variable_indices)\n",
    "print(f\"Overlap between fixed sparse subset and top {num_top_angles} high-variability: {len(overlap)} angles\")\n",
    "print(f\"Overlap indices: {overlap}\")\n",
    "\n",
    "# Create 3D sphere plot\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot sphere surface (semi-transparent)\n",
    "ax.plot_surface(X, Y, Z, color='lightblue', alpha=0.05)\n",
    "\n",
    "# Plot all projection angles (light gray, small)\n",
    "ax.scatter(x_all, y_all, z_all, color='lightgray', s=15, alpha=0.3, label='All angles')\n",
    "\n",
    "# Plot fixed sparse subset (blue)\n",
    "ax.scatter(x_sparse, y_sparse, z_sparse, color='blue', s=80, \n",
    "           alpha=0.7, edgecolors='darkblue', linewidths=1.5,\n",
    "           label=f'Fixed sparse subset ({len(fixed_sparse_indices)})')\n",
    "\n",
    "# Plot high-variability angles (red) - these will appear on top\n",
    "ax.scatter(x_high_var, y_high_var, z_high_var, color='red', s=120, \n",
    "           alpha=0.9, edgecolors='darkred', linewidths=2,\n",
    "           label=f'Top {num_top_angles} highest variability', marker='^')\n",
    "\n",
    "# Highlight overlap if any (yellow stars)\n",
    "if len(overlap) > 0:\n",
    "    x_overlap = x_all[overlap]\n",
    "    y_overlap = y_all[overlap]\n",
    "    z_overlap = z_all[overlap]\n",
    "    ax.scatter(x_overlap, y_overlap, z_overlap, color='yellow', s=150,\n",
    "               alpha=1.0, edgecolors='orange', linewidths=2,\n",
    "               label=f'Overlap ({len(overlap)})', marker='*', zorder=10)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_zlabel('Z', fontsize=11)\n",
    "ax.set_title(f'Projection Angles: Fixed Sparse Subset vs High-Variability Angles', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "ax.view_init(elev=10, azim=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another view focusing only on high-variability angles\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create sphere surface\n",
    "ax.plot_surface(X, Y, Z, color='lightgray', alpha=0.1)\n",
    "\n",
    "# Plot only high-variability angles with color scale based on variability\n",
    "scatter = ax.scatter(x_high_var, y_high_var, z_high_var, \n",
    "                     c=top_variable_values, s=200, cmap='hot',\n",
    "                     alpha=0.9, edgecolors='black', linewidths=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, pad=0.1)\n",
    "cbar.set_label('Variability', fontsize=12)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=10)\n",
    "ax.set_ylabel('Y', fontsize=10)\n",
    "ax.set_zlabel('Z', fontsize=10)\n",
    "ax.set_title(f'Top {num_top_angles} Highest-Variability Angles\\n(Color-coded by variability)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.view_init(elev=10, azim=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354cd0e",
   "metadata": {},
   "source": [
    "### Compare Projections at Most and Least Variable Angles\n",
    "\n",
    "Visualize the actual projection images at angles with highest and lowest variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c799c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few angles to visualize in detail\n",
    "num_angles_to_show = 3\n",
    "detector_angle_idx = 0  # Show one detector angle\n",
    "\n",
    "# Most variable angles\n",
    "high_var_angles = top_variable_indices[:num_angles_to_show]\n",
    "high_var_labels = [f\"Angle {idx}\\nVar={variability_per_angle[idx]:.2f}\" \n",
    "                   for idx in high_var_angles]\n",
    "\n",
    "# Least variable angles  \n",
    "low_var_angles = least_variable_indices[:num_angles_to_show]\n",
    "low_var_labels = [f\"Angle {idx}\\nVar={variability_per_angle[idx]:.2f}\" \n",
    "                  for idx in low_var_angles]\n",
    "\n",
    "print(f\"Visualizing detector angle index {detector_angle_idx}\")\n",
    "print(f\"\\nMost variable angles: {high_var_angles}\")\n",
    "print(f\"Least variable angles: {low_var_angles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d26cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot standard deviation projections for high-variability angles\n",
    "fig, axes = plt.subplots(1, num_angles_to_show, figsize=(18, 6))\n",
    "\n",
    "for i, (angle_idx, label) in enumerate(zip(high_var_angles, high_var_labels)):\n",
    "    std_proj = std_per_angle[angle_idx, :, :, detector_angle_idx]\n",
    "    \n",
    "    im = axes[i].imshow(std_proj.T, cmap='hot', origin='lower')\n",
    "    axes[i].set_title(f'High Variability\\n{label}', fontsize=11, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Standard Deviation of Projections - Most Variable Angles (Detector angle {detector_angle_idx})',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot standard deviation projections for low-variability angles\n",
    "fig, axes = plt.subplots(1, num_angles_to_show, figsize=(18, 6))\n",
    "\n",
    "for i, (angle_idx, label) in enumerate(zip(low_var_angles, low_var_labels)):\n",
    "    std_proj = std_per_angle[angle_idx, :, :, detector_angle_idx]\n",
    "    \n",
    "    im = axes[i].imshow(std_proj.T, cmap='hot', origin='lower')\n",
    "    axes[i].set_title(f'Low Variability\\n{label}', fontsize=11, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Standard Deviation of Projections - Least Variable Angles (Detector angle {detector_angle_idx})',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a924bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean projections from experiments vs ground truth for highest variability angle\n",
    "most_variable_angle = top_variable_indices[0]\n",
    "\n",
    "# Compute mean projection across experiments\n",
    "mean_projection = np.mean(all_forward_projections[:, most_variable_angle, :, :, detector_angle_idx], axis=0)\n",
    "\n",
    "# Get ground truth projection (need to project ground truth)\n",
    "print(f\"Computing ground truth projection for angle {most_variable_angle}...\")\n",
    "gt_spatial_proj = projector_full.forward(ground_truth.astype(np.float64))\n",
    "gt_forward_proj = basis_set.forward(gt_spatial_proj)\n",
    "gt_projection = gt_forward_proj[most_variable_angle, :, :, detector_angle_idx]\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "vmin = min(mean_projection.min(), gt_projection.min())\n",
    "vmax = max(mean_projection.max(), gt_projection.max())\n",
    "\n",
    "# Mean projection\n",
    "im0 = axes[0].imshow(mean_projection.T, cmap='viridis', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[0].set_title(f'Mean Projection\\nAngle {most_variable_angle}', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Ground truth projection\n",
    "im1 = axes[1].imshow(gt_projection.T, cmap='viridis', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[1].set_title(f'Ground Truth Projection\\nAngle {most_variable_angle}', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Difference\n",
    "diff = mean_projection - gt_projection\n",
    "im2 = axes[2].imshow(diff.T, cmap='RdBu_r', origin='lower',\n",
    "                     vmin=-np.abs(diff).max(), vmax=np.abs(diff).max())\n",
    "axes[2].set_title('Difference\\n(Mean - Ground Truth)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.suptitle(f'Most Variable Angle (#{most_variable_angle}, variability={variability_per_angle[most_variable_angle]:.2f})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMSE between mean and ground truth projection: {np.mean((mean_projection - gt_projection)**2):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2052f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
